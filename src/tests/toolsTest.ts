import { OpenAIClient } from '../ai/models/clients/OpenAiClient';
import { AnthropicClient } from '../ai/models/clients/AnthropicClient';
import { FireworkClient } from '../ai/models/clients/FireworkClient';
import { BaseAgent } from '../ai/agents/BaseAgent';
import { AgentConfig } from '../ai/types/agentSystem';
import { Tool } from '../ai/types/agentSystem';
import { z } from 'zod';
import { Logger } from '../utils/logger';

// Logger.enable();

// Define test tool schema
const testToolSchema = z.object({
  model_name: z.string(),
  test_message: z.string(),
  success: z.boolean(),
});
// Define test tool
const TestTool: Tool = {
  type: 'function',
  function: {
    name: 'test_model',
    description: 'Test the model functionality by generating a response',
    parameters: {
      type: 'object',
      required: ['model_name', 'test_message', 'success'],
      properties: {
        model_name: {
          type: 'string',
          description: 'Name of the model being tested',
        },
        test_message: {
          type: 'string',
          description: 'Test message generated by the model',
        },
        success: {
          type: 'boolean',
          description: 'Whether the test was successful',
        },
      },
      additionalProperties: false,
    },
  },
};

// Test agent configuration following standard agent config pattern
const testAgentConfig: AgentConfig = {
  systemPromptTemplate: `
# PERSONALITY
You are a test agent designed to verify the functionality of different AI models.

# MAIN GOAL
Test the functionality of the AI model by generating a coherent response.

# OUTPUT FORMAT
You must use the test_model function to report your test results.

## DYNAMIC VARIABLES
{{testPurpose}}
`,
  dynamicVariables: {
    testPurpose: 'Verify model functionality',
  },
};

// Test agent implementation
class TestAgent extends BaseAgent<typeof testToolSchema> {
  constructor(modelClient: any) {
    super(testAgentConfig, modelClient, testToolSchema);
  }

  protected defineTools(): void {
    this.tools = [TestTool];
    console.log('üõ†Ô∏è Test Tools Defined:', this.tools.map(t => t.function.name));
  }
}

// Main test function
async function runModelTests() {
  console.log('\nüß™ Starting Model Tests\n');

  // Test configurations
  const models = [
    {
      name: 'OpenAI',
      client: new OpenAIClient('gpt-4o', { temperature: 1 }),
      prompt: 'Generate a creative one-line story.',
    },
    {
      name: 'Anthropic',
      client: new AnthropicClient('claude-3-5-haiku-20241022', { temperature: 1 }),
      prompt: 'Generate a creative one-line story.',
    },
    {
      name: 'Fireworks',
      client: new FireworkClient('accounts/fireworks/models/llama-v3p1-405b-instruct', { temperature: 1 }),
      prompt: 'Generate a creative one-line story.',
    },
  ];

  // Run tests for each model
  for (const model of models) {
    console.log(`\nüîç Testing ${model.name} Model`);
    
    try {
      const agent = new TestAgent(model.client);

      // Run the test
      const result = await agent.run(model.prompt);
      
      if (result.success) {
        console.log(`‚úÖ ${model.name} Test Result:`, {
          modelName: result.output.model_name,
          message: result.output.test_message,
          success: result.output.success
        });
      } else {
        console.error(`‚ùå ${model.name} Test Failed:`, result.error);
      }
    } catch (error) {
      console.error(`‚ùå ${model.name} Test Failed:`, error);
    }
  }

  console.log('\nüèÅ Model Tests Completed\n');
}

// Run the tests
runModelTests();
